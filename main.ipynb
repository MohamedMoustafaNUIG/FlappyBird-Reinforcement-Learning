{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8b902e69207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import wrapped_flappy_bird as game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.num_of_actions = 2\n",
    "        self.gamma = 0.99\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.initial_epsilon = 0.1\n",
    "        self.num_of_iter = 2000000\n",
    "        self.replay_mem_size = 10000\n",
    "        self.minibatch_size = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, self.num_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters, lr=1e-6)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = game.GameState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.zeros([model.num_of_actions], dtype=torch.float32)\n",
    "action[0] = 1\n",
    "image_data, reward, terminal = game_state.frame_step(action)\n",
    "image_data = resize_and_bgr2gray(image_data)\n",
    "image_data = image_to_tensor(image_data)\n",
    "state = torch.cat((image_data, image_data, image_data, image_data)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = model.initial_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while iteration < model.number_of_iterations:\n",
    "    # get output from the neural network\n",
    "    output = model(state)[0]\n",
    "\n",
    "    # initialize action\n",
    "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
    "    if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
    "        action = action.cuda()\n",
    "\n",
    "    # epsilon greedy exploration\n",
    "    random_action = random.random() <= epsilon\n",
    "    if random_action:\n",
    "        print(\"Performed random action!\")\n",
    "    action_index = [torch.randint(model.number_of_actions, torch.Size([]), dtype=torch.int) if random_action else torch.argmax(output)][0]\n",
    "\n",
    "    if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
    "        action_index = action_index.cuda()\n",
    "\n",
    "    action[action_index] = 1\n",
    "\n",
    "    # get next state and reward\n",
    "    image_data_1, reward, terminal = game_state.frame_step(action)\n",
    "    image_data_1 = resize_and_bgr2gray(image_data_1)\n",
    "    image_data_1 = image_to_tensor(image_data_1)\n",
    "    state_1 = torch.cat((state.squeeze(0)[1:, :, :], image_data_1)).unsqueeze(0)\n",
    "\n",
    "    action = action.unsqueeze(0)\n",
    "    reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "\n",
    "    # save transition to replay memory\n",
    "    replay_memory.append((state, action, reward, state_1, terminal))\n",
    "\n",
    "    # if replay memory is full, remove the oldest transition\n",
    "    if len(replay_memory) > model.replay_memory_size:\n",
    "        replay_memory.pop(0)\n",
    "\n",
    "    # epsilon annealing\n",
    "    epsilon = epsilon_decrements[iteration]\n",
    "\n",
    "    # sample random minibatch\n",
    "    minibatch = random.sample(replay_memory, min(len(replay_memory), model.minibatch_size))\n",
    "\n",
    "    # unpack minibatch\n",
    "    state_batch = torch.cat(tuple(d[0] for d in minibatch))\n",
    "    action_batch = torch.cat(tuple(d[1] for d in minibatch))\n",
    "    reward_batch = torch.cat(tuple(d[2] for d in minibatch))\n",
    "    state_1_batch = torch.cat(tuple(d[3] for d in minibatch))\n",
    "\n",
    "    if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
    "        state_batch = state_batch.cuda()\n",
    "        action_batch = action_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        state_1_batch = state_1_batch.cuda()\n",
    "\n",
    "    # get output for the next state\n",
    "    output_1_batch = model(state_1_batch)\n",
    "\n",
    "    # set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q)\n",
    "    y_batch = torch.cat(tuple(reward_batch[i] if minibatch[i][4]\n",
    "                              else reward_batch[i] + model.gamma * torch.max(output_1_batch[i])\n",
    "                              for i in range(len(minibatch))))\n",
    "\n",
    "    # extract Q-value\n",
    "    q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
    "\n",
    "    # PyTorch accumulates gradients by default, so they need to be reset in each pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y_batch = y_batch.detach()\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(q_value, y_batch)\n",
    "\n",
    "    # do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # set state to be state_1\n",
    "    state = state_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
